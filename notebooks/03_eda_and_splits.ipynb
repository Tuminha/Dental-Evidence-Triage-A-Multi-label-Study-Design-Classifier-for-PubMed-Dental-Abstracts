{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - EDA and Temporal Splits\n",
        "\n",
        "## Goal\n",
        "\n",
        "EDA for trust: class balance, year drift, label co-occurrence. Then temporal splits (train ‚â§2021, val 2022‚Äì2023, test ‚â•2024) to prevent time leakage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Step Matters\n",
        "\n",
        "**Trust in data** comes from understanding it:\n",
        "\n",
        "- **Class balance:** Are some labels extremely rare?\n",
        "- **Year trends:** Are study designs changing over time?\n",
        "- **Co-occurrence:** Do certain labels always appear together?\n",
        "- **Temporal splits:** Prevent leakage (future knowledge influencing past predictions)\n",
        "\n",
        "Without EDA, you're training blind.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import libraries\n",
        "# Hint: import pandas as pd, numpy as np\n",
        "# import matplotlib.pyplot as plt, seaborn as sns\n",
        "# from pathlib import Path\n",
        "# import pandera as pa\n",
        "# sns.set_style('whitegrid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load processed parquet\n",
        "# Hint: df = pd.read_parquet('../data/processed/dental_abstracts.parquet')\n",
        "#       print(f\"Loaded {len(df)} papers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Counts\n",
        "\n",
        "Let's understand the dataset size and temporal distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Basic counts\n",
        "# Hint: print(f\"Total papers: {len(df)}\")\n",
        "#       print(f\"\\nYear distribution:\")\n",
        "#       print(df['year'].value_counts().sort_index())\n",
        "#       print(f\"\\nTop 10 journals:\")\n",
        "#       print(df['journal'].value_counts().head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class Balance\n",
        "\n",
        "Which labels are common? Which are rare?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Class balance barplot\n",
        "# Hint: from collections import Counter\n",
        "# all_labels = [label for labels in df['labels'] for label in labels]\n",
        "# label_counts = Counter(all_labels)\n",
        "# \n",
        "# plt.figure(figsize=(12, 6))\n",
        "# labels_sorted = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "# plt.barh([l[0] for l in labels_sorted], [l[1] for l in labels_sorted])\n",
        "# plt.xlabel('Count')\n",
        "# plt.title('Label Distribution (Multi-label)')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Label Co-occurrence\n",
        "\n",
        "Do certain labels always appear together?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Label co-occurrence heatmap\n",
        "# Hint: # Build binary matrix\n",
        "# unique_labels = sorted(label_counts.keys())\n",
        "# label_matrix = np.zeros((len(df), len(unique_labels)))\n",
        "# for i, labels in enumerate(df['labels']):\n",
        "#     for label in labels:\n",
        "#         j = unique_labels.index(label)\n",
        "#         label_matrix[i, j] = 1\n",
        "# \n",
        "# # Co-occurrence matrix\n",
        "# co_occur = label_matrix.T @ label_matrix\n",
        "# np.fill_diagonal(co_occur, 0)  # Zero out diagonal\n",
        "# \n",
        "# plt.figure(figsize=(10, 8))\n",
        "# sns.heatmap(co_occur, xticklabels=unique_labels, yticklabels=unique_labels, annot=True, fmt='.0f', cmap='Blues')\n",
        "# plt.title('Label Co-occurrence Matrix')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Temporal Splits\n",
        "\n",
        "**Critical:** Split by year to prevent temporal leakage.\n",
        "\n",
        "- **Train:** ‚â§ 2021 (~60-70% of data)\n",
        "- **Val:** 2022-2023 (~15-20%)\n",
        "- **Test:** ‚â• 2024 (~15-20%)\n",
        "\n",
        "This mimics real-world deployment: predicting future papers based on past patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Decide splits\n",
        "# Hint: def assign_split(year):\n",
        "#     if year <= 2021:\n",
        "#         return 'train'\n",
        "#     elif year <= 2023:\n",
        "#         return 'val'\n",
        "#     else:\n",
        "#         return 'test'\n",
        "# \n",
        "# df['split'] = df['year'].apply(assign_split)\n",
        "# print(df['split'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Save split parquets\n",
        "# Hint: for split_name in ['train', 'val', 'test']:\n",
        "#     split_df = df[df['split'] == split_name]\n",
        "#     split_df.to_parquet(f'../data/processed/{split_name}.parquet', index=False)\n",
        "#     print(f\"Saved {split_name}.parquet ({len(split_df)} papers)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schema Validation\n",
        "\n",
        "Use Pandera to validate data quality before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Schema validation (pandera)\n",
        "# Hint: schema = pa.DataFrameSchema({\n",
        "#     'pmid': pa.Column(str, nullable=False),\n",
        "#     'title': pa.Column(str, nullable=False, checks=pa.Check.str_length(min_value=1)),\n",
        "#     'abstract': pa.Column(str, nullable=False, checks=pa.Check.str_length(min_value=10)),\n",
        "#     'year': pa.Column(int, checks=pa.Check.in_range(2018, 2025)),\n",
        "#     'labels': pa.Column(object, checks=pa.Check(lambda x: len(x) > 0)),\n",
        "#     'split': pa.Column(str, checks=pa.Check.isin(['train', 'val', 'test']))\n",
        "# })\n",
        "# \n",
        "# # Validate\n",
        "# try:\n",
        "#     schema.validate(df)\n",
        "#     print(\"‚úÖ Schema validation passed!\")\n",
        "# except pa.errors.SchemaError as e:\n",
        "#     print(f\"‚ùå Schema validation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendations\n",
        "\n",
        "- **Revisit year cutoffs** if val/test too small (aim for at least 1000 papers each)\n",
        "- **For severe imbalance:** Consider stratified sampling within year windows (stretch goal)\n",
        "- **Document decisions:** Why these splits? What assumptions are we making?\n",
        "\n",
        "## üßò Reflection Log\n",
        "\n",
        "**What did you learn in this session?**\n",
        "- \n",
        "\n",
        "**What challenges did you encounter?**\n",
        "- \n",
        "\n",
        "**How will this improve Periospot AI?**\n",
        "- \n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
