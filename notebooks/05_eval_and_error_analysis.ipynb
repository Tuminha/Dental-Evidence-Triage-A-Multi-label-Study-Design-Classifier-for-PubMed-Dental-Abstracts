{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 - Evaluation and Error Analysis\n",
        "\n",
        "## Goal\n",
        "\n",
        "Evaluate on held-out test set and do quick error analysis to learn where the model fails.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import libraries\n",
        "# Hint: import pandas as pd, numpy as np\n",
        "# import matplotlib.pyplot as plt, seaborn as sns\n",
        "# from pathlib import Path\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "# import torch\n",
        "# from sklearn.metrics import classification_report, precision_recall_curve, average_precision_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define LABELS (same order as training!)\n",
        "# Hint: LABELS = [\n",
        "#     'SystematicReview', 'MetaAnalysis', 'RCT', 'ClinicalTrial',\n",
        "#     'Cohort', 'CaseControl', 'CaseReport', 'InVitro', 'Animal', 'Human'\n",
        "# ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model & Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load model & tokenizer\n",
        "# Hint: model_path = '../artifacts/model/best'\n",
        "#       tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "#       model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "#       model.eval()\n",
        "#       print(f\"âœ… Loaded model from {model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load test set\n",
        "# Hint: test_df = pd.read_parquet('../data/processed/test.parquet')\n",
        "#       test_df['text'] = (test_df['title'] + ' ' + test_df['abstract']).str[:2000]\n",
        "#       print(f\"Test set: {len(test_df)} papers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict Probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Predict probabilities\n",
        "# Hint: def predict_probs(texts, batch_size=16):\n",
        "#     probs_list = []\n",
        "#     for i in range(0, len(texts), batch_size):\n",
        "#         batch = texts[i:i+batch_size]\n",
        "#         inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(**inputs)\n",
        "#         probs = torch.sigmoid(outputs.logits).numpy()\n",
        "#         probs_list.append(probs)\n",
        "#     return np.vstack(probs_list)\n",
        "# \n",
        "# test_probs = predict_probs(test_df['text'].tolist())\n",
        "# print(f\"Predictions shape: {test_probs.shape}\")  # (n_test, 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Threshold Selection\n",
        "\n",
        "Default: 0.5 global threshold. For better performance, tune per-label on validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Apply threshold\n",
        "# Hint: threshold = 0.5\n",
        "#       test_preds = (test_probs > threshold).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Build true labels matrix\n",
        "# Hint: def binarize_labels(labels_list):\n",
        "#     vec = np.zeros(len(LABELS), dtype=int)\n",
        "#     for label in labels_list:\n",
        "#         if label in LABELS:\n",
        "#             vec[LABELS.index(label)] = 1\n",
        "#     return vec\n",
        "# \n",
        "# test_true = np.array([binarize_labels(labels) for labels in test_df['labels']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregate Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Metrics\n",
        "# Hint: from sklearn.metrics import precision_recall_fscore_support\n",
        "# \n",
        "# precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
        "#     test_true, test_preds, average='micro', zero_division=0\n",
        "# )\n",
        "# precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "#     test_true, test_preds, average='macro', zero_division=0\n",
        "# )\n",
        "# \n",
        "# print(\"=== Test Set Metrics (Threshold: 0.5) ===\")\n",
        "# print(f\"Micro-Precision: {precision_micro:.3f}\")\n",
        "# print(f\"Micro-Recall:    {recall_micro:.3f}\")\n",
        "# print(f\"Micro-F1:        {f1_micro:.3f}\")\n",
        "# print(f\"\\nMacro-Precision: {precision_macro:.3f}\")\n",
        "# print(f\"Macro-Recall:    {recall_macro:.3f}\")\n",
        "# print(f\"Macro-F1:        {f1_macro:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Per-label Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Per-label report\n",
        "# Hint: print(\"\\n=== Per-label Classification Report ===\")\n",
        "# print(classification_report(test_true, test_preds, target_names=LABELS, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Analysis Samples\n",
        "\n",
        "Inspect false positives and false negatives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Error analysis samples\n",
        "# Hint: # For label 'RCT' (index 2)\n",
        "# label_idx = 2\n",
        "# label_name = LABELS[label_idx]\n",
        "# \n",
        "# # False positives\n",
        "# fp_mask = (test_preds[:, label_idx] == 1) & (test_true[:, label_idx] == 0)\n",
        "# fp_indices = np.where(fp_mask)[0][:3]\n",
        "# \n",
        "# print(f\"\\n=== False Positives for {label_name} ===\")\n",
        "# for idx in fp_indices:\n",
        "#     print(f\"PMID: {test_df.iloc[idx]['pmid']}\")\n",
        "#     print(f\"Title: {test_df.iloc[idx]['title'][:100]}...\")\n",
        "#     print(f\"True labels: {test_df.iloc[idx]['labels']}\")\n",
        "#     print(f\"Predicted prob: {test_probs[idx, label_idx]:.3f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendations\n",
        "\n",
        "- **Inspect confusion** between ClinicalTrial vs RCT\n",
        "- Consider **merging ultra-rare labels** or reweighting if necessary\n",
        "- **Per-label threshold tuning** can improve F1 for imbalanced classes\n",
        "\n",
        "## ðŸ§˜ Reflection Log\n",
        "\n",
        "**What did you learn in this session?**\n",
        "- \n",
        "\n",
        "**What challenges did you encounter?**\n",
        "- \n",
        "\n",
        "**How will this improve Periospot AI?**\n",
        "- \n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
