{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 - Evaluation and Error Analysis\n",
        "\n",
        "## Goal\n",
        "\n",
        "Evaluate on held-out test set and do quick error analysis to learn where the model fails.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Import libraries for model evaluation.\n",
        "# Hints:\n",
        "# 1) pandas, numpy, transformers (AutoTokenizer, AutoModelForSequenceClassification)\n",
        "# 2) torch, sklearn.metrics\n",
        "# Acceptance:\n",
        "# - All imports successful\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Define LABELS (MUST match training order exactly!).\n",
        "# Acceptance:\n",
        "# - LABELS list with all 10 categories in correct order\n",
        "\n",
        "# TODO: define LABELS constant\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model & Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Load trained model and tokenizer.\n",
        "# Hints:\n",
        "# 1) Load from ../artifacts/model/best\n",
        "# 2) Set model to eval mode\n",
        "# Acceptance:\n",
        "# - tokenizer and model loaded\n",
        "# - model.eval() called\n",
        "\n",
        "# TODO: load model and tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Load test set and create text column.\n",
        "# Hints:\n",
        "# 1) Load test.parquet from ../data/processed\n",
        "# 2) Concatenate title + abstract (truncate to 2000 chars)\n",
        "# Acceptance:\n",
        "# - test_df loaded with 'text' column\n",
        "\n",
        "# TODO: load test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict Probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Generate probability predictions for test set.\n",
        "# Hints:\n",
        "# 1) Batch texts, tokenize, run through model\n",
        "# 2) Apply sigmoid to logits\n",
        "# 3) Return (n_samples, n_labels) array\n",
        "# Acceptance:\n",
        "# - Function predict_probs(texts, batch_size) -> np.array\n",
        "# - test_probs shape is (len(test_df), 10)\n",
        "\n",
        "def predict_probs(texts, batch_size=16):\n",
        "    \"\"\"Generate probability predictions for texts.\"\"\"\n",
        "    # TODO\n",
        "    raise NotImplementedError\n",
        "\n",
        "# TODO: generate test_probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Threshold Selection\n",
        "\n",
        "Default: 0.5 global threshold. For better performance, tune per-label on validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Convert probabilities to binary predictions.\n",
        "# Hints:\n",
        "# 1) Use threshold (e.g., 0.5)\n",
        "# 2) Cast to int\n",
        "# Acceptance:\n",
        "# - test_preds is binary matrix (0 or 1)\n",
        "\n",
        "# TODO: apply threshold to create test_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Build ground truth binary matrix.\n",
        "# Hints:\n",
        "# 1) Convert test_df['labels'] lists to binary vectors\n",
        "# 2) Same binarize logic as training\n",
        "# Acceptance:\n",
        "# - test_true shape matches test_preds\n",
        "\n",
        "# TODO: create test_true matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregate Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Compute and display aggregate metrics.\n",
        "# Hints:\n",
        "# 1) Use sklearn precision_recall_fscore_support\n",
        "# 2) Compute both micro and macro averages\n",
        "# 3) Print formatted results\n",
        "# Acceptance:\n",
        "# - Shows 6 metrics: precision/recall/f1 for micro and macro\n",
        "\n",
        "# TODO: compute and print metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Per-label Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Print per-label performance report.\n",
        "# Hints:\n",
        "# 1) Use sklearn classification_report\n",
        "# 2) Pass target_names=LABELS for readable output\n",
        "# Acceptance:\n",
        "# - Shows precision/recall/f1/support for each label\n",
        "\n",
        "# TODO: print classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Analysis Samples\n",
        "\n",
        "Inspect false positives and false negatives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Show false positives/negatives for selected labels.\n",
        "# Hints:\n",
        "# 1) Choose 1-2 interesting labels (e.g., RCT, SystematicReview)\n",
        "# 2) Find where pred != true using boolean masks\n",
        "# 3) Print pmid, title snippet, true labels, predicted prob\n",
        "# Acceptance:\n",
        "# - Shows 3-5 error examples with context\n",
        "\n",
        "# TODO: display error samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendations\n",
        "\n",
        "- **Inspect confusion** between ClinicalTrial vs RCT\n",
        "- Consider **merging ultra-rare labels** or reweighting if necessary\n",
        "- **Per-label threshold tuning** can improve F1 for imbalanced classes\n",
        "\n",
        "## ðŸ§˜ Reflection Log\n",
        "\n",
        "**What did you learn in this session?**\n",
        "- \n",
        "\n",
        "**What challenges did you encounter?**\n",
        "- \n",
        "\n",
        "**How will this improve Periospot AI?**\n",
        "- \n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
