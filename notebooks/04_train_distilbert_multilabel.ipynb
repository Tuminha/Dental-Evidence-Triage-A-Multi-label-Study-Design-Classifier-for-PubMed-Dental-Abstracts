{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 - Train DistilBERT Multi-label Classifier\n",
        "\n",
        "## Goal\n",
        "\n",
        "Train DistilBERT for multi-label classification on title+abstract. We'll use BCEWithLogits, monitor micro/macro F1, and tune threshold on the validation set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Multi-label Classification?\n",
        "\n",
        "Each paper can have multiple study characteristics:\n",
        "- An **RCT** is also **Human**\n",
        "- A **Systematic Review** might be a **MetaAnalysis**\n",
        "- Some studies combine **Animal** + **InVitro**\n",
        "\n",
        "We use **binary cross-entropy** (BCE) for each label independently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Import libraries for transformer training.\n",
        "# Hints:\n",
        "# 1) pandas, numpy, Path, torch\n",
        "# 2) From transformers: AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "# 3) From datasets: Dataset\n",
        "# Acceptance:\n",
        "# - All imports successful\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Define canonical label list (MUST match order from notebook 02).\n",
        "# Hints:\n",
        "# 1) 10 labels in this exact order for consistency\n",
        "# 2) Store as LABELS constant and compute NUM_LABELS\n",
        "# Acceptance:\n",
        "# - LABELS list with 10 study-design categories\n",
        "# - NUM_LABELS = 10\n",
        "# - Print for verification\n",
        "\n",
        "# TODO: define LABELS and NUM_LABELS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Splits & Build HF Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Load train/val/test splits and create 'text' column.\n",
        "# Hints:\n",
        "# 1) Load three parquet files from ../data/processed\n",
        "# 2) Concatenate title + ' ' + abstract into 'text' column\n",
        "# 3) Truncate to reasonable length (e.g., 2000 chars)\n",
        "# Acceptance:\n",
        "# - train_df, val_df, test_df loaded\n",
        "# - Each has 'text' column\n",
        "# - Print counts\n",
        "\n",
        "# TODO: load splits and create text column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Binarize Labels\n",
        "\n",
        "Convert list of labels â†’ multi-hot binary vector.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Convert labels lists to multi-hot binary vectors.\n",
        "# Hints:\n",
        "# 1) Create zero vector of length NUM_LABELS\n",
        "# 2) Set index to 1.0 for each label in LABELS\n",
        "# 3) Apply to all three DataFrames\n",
        "# Acceptance:\n",
        "# - Function binarize_labels(labels_list) -> list of floats\n",
        "# - New column 'label_vec' in all DataFrames\n",
        "# - Vector length = NUM_LABELS\n",
        "\n",
        "def binarize_labels(labels_list):\n",
        "    \"\"\"Convert list of labels to multi-hot vector.\"\"\"\n",
        "    # TODO\n",
        "    raise NotImplementedError\n",
        "\n",
        "# TODO: create label_vec columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Create HuggingFace Dataset objects.\n",
        "# Hints:\n",
        "# 1) Use Dataset.from_pandas() with just 'text' and 'label_vec' columns\n",
        "# 2) Create for all three splits\n",
        "# Acceptance:\n",
        "# - train_dataset, val_dataset, test_dataset created\n",
        "# - Each contains 'text' and 'label_vec' fields\n",
        "\n",
        "# TODO: convert to HF Dataset objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer & Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Tokenize datasets and prepare for training.\n",
        "# Hints:\n",
        "# 1) Load tokenizer for 'distilbert-base-uncased'\n",
        "# 2) Create tokenize function with max_length=512, truncation, padding\n",
        "# 3) Map tokenize function to all datasets (batched=True)\n",
        "# 4) Rename 'label_vec' â†’ 'labels', set format to torch\n",
        "# Acceptance:\n",
        "# - tokenizer loaded\n",
        "# - All datasets have input_ids, attention_mask, labels\n",
        "# - Format set to 'torch'\n",
        "\n",
        "# TODO: tokenize and format datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Initialize DistilBERT for multi-label classification.\n",
        "# Hints:\n",
        "# 1) Use AutoModelForSequenceClassification.from_pretrained\n",
        "# 2) Set num_labels=NUM_LABELS, problem_type='multi_label_classification'\n",
        "# Acceptance:\n",
        "# - model initialized from 'distilbert-base-uncased'\n",
        "# - Configured for multi-label (uses BCEWithLogits loss)\n",
        "\n",
        "# TODO: initialize model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Define metrics function for Trainer.\n",
        "# Hints:\n",
        "# 1) Apply sigmoid to logits, threshold at 0.5\n",
        "# 2) Compute micro/macro precision, recall, F1 using sklearn\n",
        "# 3) Return dict with 6 metrics\n",
        "# Acceptance:\n",
        "# - Function compute_metrics(eval_pred) -> dict\n",
        "# - Returns precision/recall/f1 for both micro and macro\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute evaluation metrics for multi-label classification.\"\"\"\n",
        "    # TODO\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Arguments & Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Configure training parameters.\n",
        "# Hints:\n",
        "# 1) Set output_dir, eval_strategy, save_strategy, learning_rate\n",
        "# 2) batch_size=8, epochs=3-4, warmup_ratio=0.1\n",
        "# 3) load_best_model_at_end=True, metric_for_best_model='f1_micro'\n",
        "# Acceptance:\n",
        "# - TrainingArguments object configured\n",
        "# - Will save to ../artifacts/model/checkpoints\n",
        "# - Evaluates each epoch and keeps best\n",
        "\n",
        "# TODO: create training_args\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Train model and save best checkpoint.\n",
        "# Hints:\n",
        "# 1) Create Trainer with model, args, datasets, compute_metrics\n",
        "# 2) Call trainer.train()\n",
        "# 3) Save best model and tokenizer to ../artifacts/model/best\n",
        "# Acceptance:\n",
        "# - Training completes successfully\n",
        "# - Best model saved based on micro-F1\n",
        "# - Both model and tokenizer saved\n",
        "\n",
        "# TODO: create Trainer, train, and save\n",
        "\n",
        "# Run training (uncomment when ready)\n",
        "# trainer = ...\n",
        "# trainer.train()\n",
        "# Save best model...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendations\n",
        "\n",
        "- If rare labels underperform, try **class weights** or **focal loss** (stretch goal)\n",
        "- Track **ROC/PR curves** on val to tune per-label thresholds later\n",
        "- Monitor for **overfitting:** val metrics should not degrade while train improves\n",
        "\n",
        "## ðŸ§˜ Reflection Log\n",
        "\n",
        "**What did you learn in this session?**\n",
        "- \n",
        "\n",
        "**What challenges did you encounter?**\n",
        "- \n",
        "\n",
        "**How will this improve Periospot AI?**\n",
        "- \n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
