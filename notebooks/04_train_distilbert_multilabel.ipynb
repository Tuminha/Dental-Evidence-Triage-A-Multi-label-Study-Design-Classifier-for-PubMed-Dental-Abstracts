{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 - Train DistilBERT Multi-label Classifier\n",
        "\n",
        "## Goal\n",
        "\n",
        "Train DistilBERT for multi-label classification on title+abstract. We'll use BCEWithLogits, monitor micro/macro F1, and tune threshold on the validation set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Multi-label Classification?\n",
        "\n",
        "Each paper can have multiple study characteristics:\n",
        "- An **RCT** is also **Human**\n",
        "- A **Systematic Review** might be a **MetaAnalysis**\n",
        "- Some studies combine **Animal** + **InVitro**\n",
        "\n",
        "We use **binary cross-entropy** (BCE) for each label independently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Canonical label list defined:\n",
            "   Number of labels: 10\n",
            "   Labels: ['SystematicReview', 'MetaAnalysis', 'RCT', 'ClinicalTrial', 'Cohort', 'CaseControl', 'CaseReport', 'InVitro', 'Animal', 'Human']\n",
            "\n",
            "ðŸ“‹ Label order (for binary vector encoding):\n",
            "   Index 0: SystematicReview\n",
            "   Index 1: MetaAnalysis\n",
            "   Index 2: RCT\n",
            "   Index 3: ClinicalTrial\n",
            "   Index 4: Cohort\n",
            "   Index 5: CaseControl\n",
            "   Index 6: CaseReport\n",
            "   Index 7: InVitro\n",
            "   Index 8: Animal\n",
            "   Index 9: Human\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Define canonical label list (MUST match order from notebook 02).\n",
        "# Hints:\n",
        "# 1) 10 labels in this exact order for consistency\n",
        "# 2) Store as LABELS constant and compute NUM_LABELS\n",
        "# Acceptance:\n",
        "# - LABELS list with 10 study-design categories\n",
        "# - NUM_LABELS = 10\n",
        "# - Print for verification\n",
        "\n",
        "# Canonical label list - MUST match the order from notebook 02\n",
        "# This order is critical for converting labels to binary vectors\n",
        "LABELS = [\n",
        "    'SystematicReview',  # 1. Systematic reviews\n",
        "    'MetaAnalysis',      # 2. Meta-analyses (quantitative synthesis)\n",
        "    'RCT',               # 3. Randomized Controlled Trials\n",
        "    'ClinicalTrial',     # 4. Non-randomized clinical trials\n",
        "    'Cohort',            # 5. Cohort studies (prospective/retrospective)\n",
        "    'CaseControl',       # 6. Case-control studies\n",
        "    'CaseReport',        # 7. Case reports / case series\n",
        "    'InVitro',           # 8. In vitro or ex vivo laboratory studies\n",
        "    'Animal',            # 9. Animal studies\n",
        "    'Human'              # 10. Human subjects (not mutually exclusive)\n",
        "]\n",
        "\n",
        "NUM_LABELS = len(LABELS)\n",
        "\n",
        "# Verification\n",
        "print(f\"âœ… Canonical label list defined:\")\n",
        "print(f\"   Number of labels: {NUM_LABELS}\")\n",
        "print(f\"   Labels: {LABELS}\")\n",
        "print(f\"\\nðŸ“‹ Label order (for binary vector encoding):\")\n",
        "for i, label in enumerate(LABELS):\n",
        "    print(f\"   Index {i}: {label}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Splits & Build HF Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acceptance criteria met\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Load train/val/test splits and create 'text' column.\n",
        "# Hints:\n",
        "# 1) Load three parquet files from ../data/processed\n",
        "# 2) Concatenate title + ' ' + abstract into 'text' column\n",
        "# 3) Truncate to reasonable length (e.g., 2000 chars)\n",
        "# Acceptance:\n",
        "# - train_df, val_df, test_df loaded\n",
        "# - Each has 'text' column\n",
        "# - Print counts\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: load splits and create text column\n",
        "processed_data_path = Path('../data/processed')\n",
        "train_df = pd.read_parquet(processed_data_path / 'train.parquet')\n",
        "val_df = pd.read_parquet(processed_data_path / 'val.parquet')\n",
        "test_df = pd.read_parquet(processed_data_path / 'test.parquet')\n",
        "\n",
        "train_df['text'] = train_df['title'] + ' ' + train_df['abstract']\n",
        "val_df['text'] = val_df['title'] + ' ' + val_df['abstract']\n",
        "test_df['text'] = test_df['title'] + ' ' + test_df['abstract']\n",
        "\n",
        "train_df['text'] = train_df['text'].str[:2000]\n",
        "val_df['text'] = val_df['text'].str[:2000]\n",
        "test_df['text'] = test_df['text'].str[:2000]\n",
        "\n",
        "def acceptance_criteria():\n",
        "    assert train_df['text'].notna().all(), \"All texts should be non-null\"\n",
        "    assert val_df['text'].notna().all(), \"All texts should be non-null\"\n",
        "    assert test_df['text'].notna().all(), \"All texts should be non-null\"\n",
        "    assert train_df['text'].str.len().max() <= 2000, \"Texts should be truncated to 2000 characters\"\n",
        "    assert val_df['text'].str.len().max() <= 2000, \"Texts should be truncated to 2000 characters\"\n",
        "    assert test_df['text'].str.len().max() <= 2000, \"Texts should be truncated to 2000 characters\"\n",
        "    assert train_df['text'].str.len().min() > 0, \"Texts should not be empty\"\n",
        "    assert val_df['text'].str.len().min() > 0, \"Texts should not be empty\"\n",
        "    assert test_df['text'].str.len().min() > 0, \"Texts should not be empty\"\n",
        "    return \"Acceptance criteria met\"\n",
        "\n",
        "print(acceptance_criteria())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Binarize Labels\n",
        "\n",
        "Convert list of labels â†’ multi-hot binary vector.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                   text   labels  \\\n",
            "1347  Stability of Class II Malocclusion Treatment w...  [Human]   \n",
            "1348  The Dental Aesthetic Index and Its Association...  [Human]   \n",
            "1349  Aboriginal Health Workers Promoting Oral Healt...  [Human]   \n",
            "1350  Sleep Bruxism in Children: Etiology, Diagnosis...  [Human]   \n",
            "1351  Orthodontic Extrusion vs. Surgical Extrusion t...  [Human]   \n",
            "\n",
            "                                              label_vec  \n",
            "1347  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "1348  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "1349  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "1350  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "1351  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "label_vec\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]    15441\n",
            "[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]     2636\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]     1810\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]     1543\n",
            "[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]     1306\n",
            "                                                      ...  \n",
            "[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]        1\n",
            "[0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]        1\n",
            "[0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]        1\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]        1\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]        1\n",
            "Name: count, Length: 98, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Convert labels lists to multi-hot binary vectors.\n",
        "# Hints:\n",
        "# 1) Create zero vector of length NUM_LABELS\n",
        "# 2) Set index to 1.0 for each label in LABELS\n",
        "# 3) Apply to all three DataFrames\n",
        "# Acceptance:\n",
        "# - Function binarize_labels(labels_list) -> list of floats\n",
        "# - New column 'label_vec' in all DataFrames\n",
        "# - Vector length = NUM_LABELS\n",
        "\n",
        "def binarize_labels(labels_list):\n",
        "    \"\"\"Convert list of labels to multi-hot vector.\"\"\"\n",
        "    # TODO\n",
        "    zero_vector = [0.0] * NUM_LABELS\n",
        "    for label in labels_list:\n",
        "        if label in LABELS:\n",
        "            zero_vector[LABELS.index(label)] = 1.0\n",
        "    return zero_vector\n",
        "\n",
        "train_df['label_vec'] = train_df['labels'].apply(binarize_labels)\n",
        "val_df['label_vec'] = val_df['labels'].apply(binarize_labels)\n",
        "test_df['label_vec'] = test_df['labels'].apply(binarize_labels)\n",
        "\n",
        "print(train_df[['text', \"labels\", 'label_vec']].head())\n",
        "print(train_df[\"label_vec\"].value_counts())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': \"Stability of Class II Malocclusion Treatment with the Austro Repositioner Followed by Fixed Appliances in Brachyfacial Patients. One of the goals of functional-appliance devices is to modify the vertical growth pattern, solving several kinds of malocclusion. This study aimed to evaluate Class II malocclusion treatment's stability with Austro Repositioner, followed by fixed appliances, and assess its capacity to modify vertical dimensions in brachyfacial patients. A test group of 30 patients (16 boys and 14 girls, mean 11.9 years old) with Class II malocclusion due to mandibular retrognathism and brachyfacial pattern treated with Austro Repositioner and fixed appliance were compared to a matched untreated Class II control group of 30 patients (17 boys and 13 girls, mean age 11.7 years old). Lateral cephalograms were taken at T1 (initial records), T2 (end of treatment), and T3 (one year after treatment). Statistical comparisons were performed with a paired-sample  t -test and two-sample  t -tests. Significant improvements in the skeletal Class II relationship were observed in the treated group. The ANB angle decreased (4.75Â°), the SNB angle increased (3.92Â°), and the total mandibular length (Co-Pg) increased (8.18 mm) ( p <  0.001). Vertical dimensions were also significantly modified, the FMA angle increased (3.94Â°), LAFH-distance increased (3.15 mm), and overbite decreased (3.35 mm). These changes remained stable one year after treatment. The Austro Repositioner was adequate for treating the skeletal Class II malocclusion resulting from the mandible retrusion in brachyfacial patients.\", 'label_vec': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], '__index_level_0__': 1347}\n",
            "{'text': 'Adherence to dietary guidelines and dental caries among children: a longitudinal cohort study. Even though dietary sugars are the most important nutrient for caries development, the disease process is dependent on other dietary practices. The intake of individual nutrient components cannot be evaluated separately from the overall diet which includes other nutrients, foods and habits. Therefore, the aim of this study was to investigate the association between adherence to dietary guidelines and dental caries. This study was embedded in the Generation R Study, conducted in Rotterdam, the Netherlands. In total, 2911 children were included in the present analyses. Dietary intake at the age of 8\\u2009years was assessed using food-frequency questionnaires. Diet quality scores were estimated, reflecting adherence to Dutch dietary guidelines. Dental caries was assessed at the age of 13\\u2009years using intra-oral photographs. Associations were estimated using multinomial logistic regression analyses, adjusted for sociodemographic characteristics and oral hygiene practices. The prevalence of dental caries at the age of 13\\u2009years was 33% (n\\u2009=\\u2009969). Better diet quality was associated with a lower occurrence of severe dental caries after adjustments for sociodemographic factors [e.g. highest vs. lowest quartile of diet quality: odds ratio (OR) 0.62, 95% confidence interval (CI) 0.39-0.98]. After additional adjustments for oral hygiene practices, this association was not statistically significant (OR 0.65, 95% CI 0.41-1.03). Adherence to dietary guidelines has the potential to reduce dental caries in children; however, with proper oral hygiene practices, this relationship might be attenuated. To understand the role of dietary patterns and dental caries, the contributing role of daily eating occasions needs to be studied further.', 'label_vec': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], '__index_level_0__': 1007}\n",
            "{'text': 'Evaluation of REFIX Technology on the Remineralization of Artificial Early Enamel Caries Lesion by Laser Speckle Tracking Analysis. To assess the effectiveness of the REFIX technology in the remineralization process of initial caries simulated on bovine enamel. The assessment involved the analysis of backscatter intensity, which was determined from laser speckle images. Twenty-one bovine teeth were divided into three groups: G1 and G7 were submitted to treatment with the REFIX technology for 1 and 7\\u2009days, respectively. The control group was treated with deionized water. A significant difference in backscatter was found between the carious and sound areas in all groups (p\\u2009=\\u20090.0038, p\\u2009<\\u20090.0001, and p\\u2009=\\u20090.0002 for the control group, G1, and G7, respectively). The intergroup comparison revealed no significant difference among the groups studied. REFIX technology did not alter the optical properties of the samples of bovine teeth with simulated initial caries lesions after 1 and 7\\u2009days of treatment.', 'label_vec': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], '__index_level_0__': 0}\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Create HuggingFace Dataset objects.\n",
        "# Hints:\n",
        "# 1) Use Dataset.from_pandas() with just 'text' and 'label_vec' columns\n",
        "# 2) Create for all three splits\n",
        "# Acceptance:\n",
        "# - train_dataset, val_dataset, test_dataset created\n",
        "# - Each contains 'text' and 'label_vec' fields\n",
        "from datasets import Dataset\n",
        "\n",
        "# TODO: convert to HF Dataset objects\n",
        "def convert_to_hf_dataset(df):\n",
        "    return Dataset.from_pandas(df[['text', 'label_vec']])\n",
        "\n",
        "train_dataset = convert_to_hf_dataset(train_df)\n",
        "val_dataset = convert_to_hf_dataset(val_df)\n",
        "test_dataset = convert_to_hf_dataset(test_df)\n",
        "\n",
        "# Test\n",
        "print(train_dataset[0])\n",
        "print(val_dataset[0])\n",
        "print(test_dataset[0])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer & Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8c7ce4f995a4b22a4d050967a4a525e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/29926 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c06061adfe0146a396e4274e1602fcf8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/16057 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a6cd02095864a7395b54545afa7d1d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/18666 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), 'input_ids': tensor([  101,  9211,  1997,  2465,  2462, 15451, 10085, 20464, 14499,  3949,\n",
            "         2007,  1996, 16951, 16360, 19234,  2121,  2628,  2011,  4964, 22449,\n",
            "         1999, 11655, 11714,  7011, 13247,  5022,  1012,  2028,  1997,  1996,\n",
            "         3289,  1997,  8360,  1011, 10439, 15204,  3401,  5733,  2003,  2000,\n",
            "        19933,  1996,  7471,  3930,  5418,  1010, 13729,  2195,  7957,  1997,\n",
            "        15451, 10085, 20464, 14499,  1012,  2023,  2817,  6461,  2000, 16157,\n",
            "         2465,  2462, 15451, 10085, 20464, 14499,  3949,  1005,  1055,  9211,\n",
            "         2007, 16951, 16360, 19234,  2121,  1010,  2628,  2011,  4964, 22449,\n",
            "         1010,  1998, 14358,  2049,  3977,  2000, 19933,  7471,  9646,  1999,\n",
            "        11655, 11714,  7011, 13247,  5022,  1012,  1037,  3231,  2177,  1997,\n",
            "         2382,  5022,  1006,  2385,  3337,  1998,  2403,  3057,  1010,  2812,\n",
            "         2340,  1012,  1023,  2086,  2214,  1007,  2007,  2465,  2462, 15451,\n",
            "        10085, 20464, 14499,  2349,  2000,  2158,  4305, 28808, 22307, 16989,\n",
            "        15222,  6491,  1998, 11655, 11714,  7011, 13247,  5418,  5845,  2007,\n",
            "        16951, 16360, 19234,  2121,  1998,  4964, 10439, 15204,  3401,  2020,\n",
            "         4102,  2000,  1037, 10349,  4895,  7913,  4383,  2465,  2462,  2491,\n",
            "         2177,  1997,  2382,  5022,  1006,  2459,  3337,  1998,  2410,  3057,\n",
            "         1010,  2812,  2287,  2340,  1012,  1021,  2086,  2214,  1007,  1012,\n",
            "        11457,  8292, 21890, 24915,  2015,  2020,  2579,  2012,  1056,  2487,\n",
            "         1006,  3988,  2636,  1007,  1010,  1056,  2475,  1006,  2203,  1997,\n",
            "         3949,  1007,  1010,  1998,  1056,  2509,  1006,  2028,  2095,  2044,\n",
            "         3949,  1007,  1012,  7778, 18539,  2020,  2864,  2007,  1037, 12739,\n",
            "         1011,  7099,  1056,  1011,  3231,  1998,  2048,  1011,  7099,  1056,\n",
            "         1011,  5852,  1012,  3278,  8377,  1999,  1996, 20415,  2465,  2462,\n",
            "         3276,  2020,  5159,  1999,  1996,  5845,  2177,  1012,  1996,  2019,\n",
            "         2497,  6466, 10548,  1006,  1018,  1012,  4293,  7737,  1007,  1010,\n",
            "         1996,  1055, 27698,  6466,  3445,  1006,  1017,  1012,  6227,  7737,\n",
            "         1007,  1010,  1998,  1996,  2561,  2158,  4305, 28808,  3091,  1006,\n",
            "         2522,  1011, 18720,  1007,  3445,  1006,  1022,  1012,  2324,  3461,\n",
            "         1007,  1006,  1052,  1026,  1014,  1012, 25604,  1007,  1012,  7471,\n",
            "         9646,  2020,  2036,  6022,  6310,  1010,  1996,  4718,  2050,  6466,\n",
            "         3445,  1006,  1017,  1012,  6365,  7737,  1007,  1010,  2474,  2546,\n",
            "         2232,  1011,  3292,  3445,  1006,  1017,  1012,  2321,  3461,  1007,\n",
            "         1010,  1998,  2058, 16313,  2063, 10548,  1006,  1017,  1012,  3486,\n",
            "         3461,  1007,  1012,  2122,  3431,  2815,  6540,  2028,  2095,  2044,\n",
            "         3949,  1012,  1996, 16951, 16360, 19234,  2121,  2001, 11706,  2005,\n",
            "        12318,  1996, 20415,  2465,  2462, 15451, 10085, 20464, 14499,  4525,\n",
            "         2013,  1996,  2158,  4305,  3468,  2128, 16344, 14499,  1999, 11655,\n",
            "        11714,  7011, 13247,  5022,  1012,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])}\n",
            "{'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), 'input_ids': tensor([  101, 29235,  2000, 23444, 11594,  1998, 11394,  2482,  3111,  2426,\n",
            "         2336,  1024,  1037, 20134,  2522, 27794,  2817,  1012,  2130,  2295,\n",
            "        23444,  5699,  2015,  2024,  1996,  2087,  2590, 26780,  2005,  2482,\n",
            "         3111,  2458,  1010,  1996,  4295,  2832,  2003,  7790,  2006,  2060,\n",
            "        23444,  6078,  1012,  1996, 13822,  1997,  3265, 26780,  6177,  3685,\n",
            "         2022, 16330, 10329,  2013,  1996,  3452,  8738,  2029,  2950,  2060,\n",
            "        20435,  1010,  9440,  1998, 14243,  1012,  3568,  1010,  1996,  6614,\n",
            "         1997,  2023,  2817,  2001,  2000,  8556,  1996,  2523,  2090, 29235,\n",
            "         2000, 23444, 11594,  1998, 11394,  2482,  3111,  1012,  2023,  2817,\n",
            "         2001, 11157,  1999,  1996,  4245,  1054,  2817,  1010,  4146,  1999,\n",
            "        15632,  1010,  1996,  4549,  1012,  1999,  2561,  1010, 27173,  2487,\n",
            "         2336,  2020,  2443,  1999,  1996,  2556, 16478,  1012, 23444, 13822,\n",
            "         2012,  1996,  2287,  1997,  1022,  2086,  2001, 14155,  2478,  2833,\n",
            "         1011,  6075,  3160, 20589,  2015,  1012,  8738,  3737,  7644,  2020,\n",
            "         4358,  1010, 10842, 29235,  2000,  3803, 23444, 11594,  1012, 11394,\n",
            "         2482,  3111,  2001, 14155,  2012,  1996,  2287,  1997,  2410,  2086,\n",
            "         2478, 26721,  1011,  8700,  7008,  1012,  8924,  2020,  4358,  2478,\n",
            "         4800,  3630, 10092,  2140,  8833,  6553, 26237, 16478,  1010, 10426,\n",
            "         2005, 17522,  3207,  5302, 14773,  6459,  1998,  8700, 19548,  6078,\n",
            "         1012,  1996, 20272,  1997, 11394,  2482,  3111,  2012,  1996,  2287,\n",
            "         1997,  2410,  2086,  2001,  3943,  1003,  1006,  1050,  1027,  5986,\n",
            "         2683,  1007,  1012,  2488,  8738,  3737,  2001,  3378,  2007,  1037,\n",
            "         2896, 14404,  1997,  5729, 11394,  2482,  3111,  2044, 24081,  2005,\n",
            "        17522,  3207,  5302, 14773,  5876,  1031,  1041,  1012,  1043,  1012,\n",
            "         3284,  5443,  1012,  7290, 24209,  8445,  9463,  1997,  8738,  3737,\n",
            "         1024, 10238,  6463,  1006,  2030,  1007,  1014,  1012,  5786,  1010,\n",
            "         5345,  1003,  7023, 13483,  1006, 25022,  1007,  1014,  1012,  4464,\n",
            "         1011,  1014,  1012,  5818,  1033,  1012,  2044,  3176, 24081,  2005,\n",
            "         8700, 19548,  6078,  1010,  2023,  2523,  2001,  2025,  7778,  2135,\n",
            "         3278,  1006,  2030,  1014,  1012,  3515,  1010,  5345,  1003, 25022,\n",
            "         1014,  1012,  4601,  1011,  1015,  1012,  6021,  1007,  1012, 29235,\n",
            "         2000, 23444, 11594,  2038,  1996,  4022,  2000,  5547, 11394,  2482,\n",
            "         3111,  1999,  2336,  1025,  2174,  1010,  2007,  5372,  8700, 19548,\n",
            "         6078,  1010,  2023,  3276,  2453,  2022,  2012,  6528, 16453,  1012,\n",
            "         2000,  3305,  1996,  2535,  1997, 23444,  7060,  1998, 11394,  2482,\n",
            "         3111,  1010,  1996,  8020,  2535,  1997,  3679,  5983,  6642,  3791,\n",
            "         2000,  2022,  3273,  2582,  1012,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])}\n",
            "{'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), 'input_ids': tensor([  101,  9312,  1997, 25416,  7646,  2974,  2006,  1996,  2128, 11233,\n",
            "         7941,  3989,  1997,  7976,  2220, 29484,  2482,  3111,  4649,  3258,\n",
            "         2011,  9138, 28699, 19099,  9651,  4106,  1012,  2000, 14358,  1996,\n",
            "        12353,  1997,  1996, 25416,  7646,  2974,  1999,  1996,  2128, 11233,\n",
            "         7941,  3989,  2832,  1997,  3988,  2482,  3111, 23599,  2006,  8945,\n",
            "        20534, 29484,  1012,  1996,  7667,  2920,  1996,  4106,  1997, 10457,\n",
            "        11266,  3334,  8015,  1010,  2029,  2001,  4340,  2013,  9138, 28699,\n",
            "        19099,  4871,  1012,  3174,  1011,  2028,  8945, 20534,  4091,  2020,\n",
            "         4055,  2046,  2093,  2967,  1024,  1043,  2487,  1998,  1043,  2581,\n",
            "         2020,  7864,  2000,  3949,  2007,  1996, 25416,  7646,  2974,  2005,\n",
            "         1015,  1998,  1021,  2420,  1010,  4414,  1012,  1996,  2491,  2177,\n",
            "         2001,  5845,  2007, 14866, 10698,  5422,  2300,  1012,  1037,  3278,\n",
            "         4489,  1999, 10457, 11266,  3334,  2001,  2179,  2090,  1996,  2482,\n",
            "         6313,  1998,  2614,  2752,  1999,  2035,  2967,  1006,  1052,  1027,\n",
            "         1014,  1012,  4002, 22025,  1010,  1052,  1026,  1014,  1012,  2199,\n",
            "         2487,  1010,  1998,  1052,  1027,  1014,  1012,  2199,  2475,  2005,\n",
            "         1996,  2491,  2177,  1010,  1043,  2487,  1010,  1998,  1043,  2581,\n",
            "         1010,  4414,  1007,  1012,  1996,  6970, 17058,  7831,  3936,  2053,\n",
            "         3278,  4489,  2426,  1996,  2967,  3273,  1012, 25416,  7646,  2974,\n",
            "         2106,  2025, 11477,  1996,  9380,  5144,  1997,  1996,  8168,  1997,\n",
            "         8945, 20534,  4091,  2007, 23599,  3988,  2482,  3111, 22520,  2044,\n",
            "         1015,  1998,  1021,  2420,  1997,  3949,  1012,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])}\n",
            "{'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids', 'attention_mask', 'labels'], 'output_all_columns': False}\n",
            "{'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids', 'attention_mask', 'labels'], 'output_all_columns': False}\n",
            "{'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids', 'attention_mask', 'labels'], 'output_all_columns': False}\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m val_dataset.num_rows == \u001b[38;5;28mlen\u001b[39m(val_df)\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m test_dataset.num_rows == \u001b[38;5;28mlen\u001b[39m(test_df)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m train_dataset.num_columns == \u001b[32m3\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m val_dataset.num_columns == \u001b[32m3\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m test_dataset.num_columns == \u001b[32m3\u001b[39m\n",
            "\u001b[31mAssertionError\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Tokenize datasets and prepare for training.\n",
        "# Hints:\n",
        "# 1) Load tokenizer for 'distilbert-base-uncased'\n",
        "# 2) Create tokenize function with max_length=512, truncation, padding\n",
        "# 3) Map tokenize function to all datasets (batched=True)\n",
        "# 4) Rename 'label_vec' â†’ 'labels', set format to torch\n",
        "# Acceptance:\n",
        "# - tokenizer loaded\n",
        "# - All datasets have input_ids, attention_mask, labels\n",
        "# - Format set to 'torch'\n",
        "\n",
        "# TODO: tokenize and format datasets\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize_and_format(dataset):\n",
        "    return dataset.map(lambda x: tokenizer(x['text'], max_length=512, truncation=True, padding='max_length'), batched=True)\n",
        "\n",
        "train_dataset = tokenize_and_format(train_dataset)\n",
        "val_dataset = tokenize_and_format(val_dataset)\n",
        "test_dataset = tokenize_and_format(test_dataset)\n",
        "\n",
        "# Rename 'label_vec' â†’ 'labels', set format to torch\n",
        "train_dataset = train_dataset.rename_column('label_vec', 'labels')\n",
        "val_dataset = val_dataset.rename_column('label_vec', 'labels')\n",
        "test_dataset = test_dataset.rename_column('label_vec', 'labels')\n",
        "\n",
        "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Test\n",
        "print(train_dataset[0])\n",
        "print(val_dataset[0])\n",
        "print(test_dataset[0])\n",
        "\n",
        "print(train_dataset.format)\n",
        "print(val_dataset.format)\n",
        "print(test_dataset.format)\n",
        "\n",
        "# Acceptance\n",
        "assert train_dataset.format is not None and train_dataset.format['type'] == 'torch'\n",
        "assert val_dataset.format is not None and val_dataset.format['type'] == 'torch'\n",
        "assert test_dataset.format is not None and test_dataset.format['type'] == 'torch'\n",
        "assert 'labels' in train_dataset.column_names\n",
        "assert 'labels' in val_dataset.column_names\n",
        "assert 'labels' in test_dataset.column_names\n",
        "assert train_dataset.num_rows == len(train_df)\n",
        "assert val_dataset.num_rows == len(val_df)\n",
        "assert test_dataset.num_rows == len(test_df)\n",
        "assert train_dataset.num_columns == 3\n",
        "assert val_dataset.num_columns == 3\n",
        "assert test_dataset.num_columns == 3\n",
        "assert train_dataset.num_columns == val_dataset.num_columns == test_dataset.num_columns\n",
        "assert train_dataset.num_rows == val_dataset.num_rows == test_dataset.num_rows\n",
        "assert train_dataset.columns[0] == 'input_ids'\n",
        "assert val_dataset.columns[0] == 'input_ids'\n",
        "assert test_dataset.columns[0] == 'input_ids'\n",
        "assert train_dataset.columns[1] == 'attention_mask'\n",
        "assert val_dataset.columns[1] == 'attention_mask'\n",
        "assert test_dataset.columns[1] == 'attention_mask'\n",
        "assert train_dataset.columns[2] == 'labels'\n",
        "assert val_dataset.columns[2] == 'labels'\n",
        "assert test_dataset.columns[2] == 'labels'\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Initialize DistilBERT for multi-label classification.\n",
        "# Hints:\n",
        "# 1) Use AutoModelForSequenceClassification.from_pretrained\n",
        "# 2) Set num_labels=NUM_LABELS, problem_type='multi_label_classification'\n",
        "# Acceptance:\n",
        "# - model initialized from 'distilbert-base-uncased'\n",
        "# - Configured for multi-label (uses BCEWithLogits loss)\n",
        "\n",
        "# TODO: initialize model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Define metrics function for Trainer.\n",
        "# Hints:\n",
        "# 1) Apply sigmoid to logits, threshold at 0.5\n",
        "# 2) Compute micro/macro precision, recall, F1 using sklearn\n",
        "# 3) Return dict with 6 metrics\n",
        "# Acceptance:\n",
        "# - Function compute_metrics(eval_pred) -> dict\n",
        "# - Returns precision/recall/f1 for both micro and macro\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute evaluation metrics for multi-label classification.\"\"\"\n",
        "    # TODO\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Arguments & Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Configure training parameters.\n",
        "# Hints:\n",
        "# 1) Set output_dir, eval_strategy, save_strategy, learning_rate\n",
        "# 2) batch_size=8, epochs=3-4, warmup_ratio=0.1\n",
        "# 3) load_best_model_at_end=True, metric_for_best_model='f1_micro'\n",
        "# Acceptance:\n",
        "# - TrainingArguments object configured\n",
        "# - Will save to ../artifacts/model/checkpoints\n",
        "# - Evaluates each epoch and keeps best\n",
        "\n",
        "# TODO: create training_args\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Train model and save best checkpoint.\n",
        "# Hints:\n",
        "# 1) Create Trainer with model, args, datasets, compute_metrics\n",
        "# 2) Call trainer.train()\n",
        "# 3) Save best model and tokenizer to ../artifacts/model/best\n",
        "# Acceptance:\n",
        "# - Training completes successfully\n",
        "# - Best model saved based on micro-F1\n",
        "# - Both model and tokenizer saved\n",
        "\n",
        "# TODO: create Trainer, train, and save\n",
        "\n",
        "# Run training (uncomment when ready)\n",
        "# trainer = ...\n",
        "# trainer.train()\n",
        "# Save best model...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendations\n",
        "\n",
        "- If rare labels underperform, try **class weights** or **focal loss** (stretch goal)\n",
        "- Track **ROC/PR curves** on val to tune per-label thresholds later\n",
        "- Monitor for **overfitting:** val metrics should not degrade while train improves\n",
        "\n",
        "## ðŸ§˜ Reflection Log\n",
        "\n",
        "**What did you learn in this session?**\n",
        "- \n",
        "\n",
        "**What challenges did you encounter?**\n",
        "- \n",
        "\n",
        "**How will this improve Periospot AI?**\n",
        "- \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
