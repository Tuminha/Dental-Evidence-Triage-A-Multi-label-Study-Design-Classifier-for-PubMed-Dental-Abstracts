{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06 - Push to Hugging Face Hub\n",
        "\n",
        "## Goal\n",
        "\n",
        "Publish: push the model to the Hugging Face Hub with a model card, and (optionally) push a small sample dataset. Then add an inference widget.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Import libraries for Hugging Face Hub interaction.\n",
        "# Hints:\n",
        "# 1) os, Path, huggingface_hub (login, HfApi, create_repo, upload_folder)\n",
        "# Acceptance:\n",
        "# - All imports successful\n",
        "\n",
        "# TODO: import libraries\n",
        "import os\n",
        "from pathlib import Path\n",
        "from huggingface_hub import HfApi, login, create_repo, upload_folder\n",
        "from dotenv import load_dotenv\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Login & Setup\n",
        "\n",
        "Set `HUGGINGFACE_HUB_TOKEN` in your environment or pass it here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged in to Hugging Face Hub\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Authenticate with Hugging Face Hub.\n",
        "# Hints:\n",
        "# 1) Get HF token from environment variable\n",
        "# 2) Call login() if token available\n",
        "# 3) Define repo names\n",
        "# Acceptance:\n",
        "# - Logged in or message shown\n",
        "# - MODEL_REPO and DATASET_REPO defined\n",
        "\n",
        "# TODO: login and define repo names\n",
        "load_dotenv()\n",
        "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
        "\n",
        "MODEL_REPO = \"Tuminha/dental-evidence-triage\"\n",
        "DATASET_REPO = \"Tuminha/dental-evidence-dataset\"\n",
        "\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"Logged in to Hugging Face Hub\")\n",
        "else:\n",
        "    print(\"No token provided, skipping login\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Model Card\n",
        "\n",
        "Load the template and fill in your actual metrics from notebook 05.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model card created at ../artifacts/model/best/README.md\n",
            "   - YAML metadata added\n",
            "   - Micro-F1: 0.8917\n",
            "   - Macro-F1: 0.7397\n",
            "   - Per-label metrics filled for all 10 labels\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Prepare model card with actual metrics.\n",
        "# Hints:\n",
        "# 1) Load MODEL_CARD_TEMPLATE.md\n",
        "# 2) Replace [TBD] placeholders with your metrics from notebook 05\n",
        "# 3) Write to ../artifacts/model/best/README.md\n",
        "# Acceptance:\n",
        "# - README.md created with filled metrics\n",
        "\n",
        "# Load template\n",
        "template_path = Path(\"../MODEL_CARD_TEMPLATE.md\")\n",
        "with open(template_path, \"r\") as file:\n",
        "    template = file.read()\n",
        "\n",
        "# Metrics from Notebook 05 (Test Set Evaluation)\n",
        "# Aggregate metrics\n",
        "micro_f1 = 0.8917\n",
        "macro_f1 = 0.7397\n",
        "micro_precision = 0.8966\n",
        "micro_recall = 0.8868\n",
        "macro_precision = 0.8201\n",
        "macro_recall = 0.7596\n",
        "\n",
        "# Per-label metrics (from classification_report in Notebook 05)\n",
        "per_label_metrics = {\n",
        "    'SystematicReview': {'precision': 0.81, 'recall': 0.93, 'f1': 0.87, 'support': 1326},\n",
        "    'MetaAnalysis': {'precision': 0.77, 'recall': 0.97, 'f1': 0.86, 'support': 601},\n",
        "    'RCT': {'precision': 0.70, 'recall': 0.92, 'f1': 0.80, 'support': 1046},\n",
        "    'ClinicalTrial': {'precision': 0.64, 'recall': 0.28, 'f1': 0.39, 'support': 103},\n",
        "    'Cohort': {'precision': 0.69, 'recall': 0.89, 'f1': 0.78, 'support': 1768},\n",
        "    'CaseControl': {'precision': 0.89, 'recall': 0.04, 'f1': 0.08, 'support': 1513},\n",
        "    'CaseReport': {'precision': 0.95, 'recall': 0.89, 'f1': 0.92, 'support': 1409},\n",
        "    'InVitro': {'precision': 0.93, 'recall': 0.93, 'f1': 0.93, 'support': 2183},\n",
        "    'Animal': {'precision': 0.86, 'recall': 0.79, 'f1': 0.82, 'support': 1651},\n",
        "    'Human': {'precision': 0.95, 'recall': 0.96, 'f1': 0.96, 'support': 16489}\n",
        "}\n",
        "\n",
        "# Replace aggregate metrics\n",
        "template = template.replace(\"[TARGET: â‰¥0.75]\", f\"{micro_f1:.4f}\")\n",
        "template = template.replace(\"[Expected lower due to imbalance]\", f\"{macro_f1:.4f}\")\n",
        "template = template.replace(\"**Micro-Precision** | [TBD]\", f\"**Micro-Precision** | {micro_precision:.4f}\")\n",
        "template = template.replace(\"**Micro-Recall** | [TBD]\", f\"**Micro-Recall** | {micro_recall:.4f}\")\n",
        "template = template.replace(\"**Macro-Precision** | [TBD]\", f\"**Macro-Precision** | {macro_precision:.4f}\")\n",
        "template = template.replace(\"**Macro-Recall** | [TBD]\", f\"**Macro-Recall** | {macro_recall:.4f}\")\n",
        "\n",
        "# Replace per-label table\n",
        "per_label_table = \"| Label | Precision | Recall | F1 | Support |\\n\"\n",
        "per_label_table += \"|-------|-----------|--------|-----|---------|\\n\"\n",
        "for label in ['SystematicReview', 'MetaAnalysis', 'RCT', 'ClinicalTrial', 'Cohort', \n",
        "              'CaseControl', 'CaseReport', 'InVitro', 'Animal', 'Human']:\n",
        "    metrics = per_label_metrics[label]\n",
        "    per_label_table += f\"| {label} | {metrics['precision']:.2f} | {metrics['recall']:.2f} | {metrics['f1']:.2f} | {metrics['support']} |\\n\"\n",
        "\n",
        "# Find and replace the per-label table section\n",
        "import re\n",
        "# Match the entire table including header, separator, and all rows\n",
        "# Pattern matches from \"| Label | Precision...\" through the separator line to the last \"| Human |...\"\n",
        "pattern = r'\\| Label \\| Precision \\| Recall \\| F1 \\| Support \\|\\n\\|-+\\|.*?\\| Human \\| 0\\.XX \\| 0\\.XX \\| 0\\.XX \\| XXX \\|'\n",
        "replacement = per_label_table.strip()\n",
        "template = re.sub(pattern, replacement, template, flags=re.DOTALL)\n",
        "\n",
        "# Fallback: if regex didn't match, use string replacement\n",
        "if \"| Human | 0.XX\" in template:\n",
        "    # Find the table start (header) and end (last row)\n",
        "    start_marker = \"| Label | Precision | Recall | F1 | Support |\"\n",
        "    end_marker = \"| Human | 0.XX | 0.XX | 0.XX | XXX |\"\n",
        "    start_idx = template.find(start_marker)\n",
        "    if start_idx != -1:\n",
        "        # Find the end of the table (after the last row)\n",
        "        end_idx = template.find(end_marker, start_idx)\n",
        "        if end_idx != -1:\n",
        "            end_idx = template.find(\"\\n\", end_idx) + 1  # Include the newline after the last row\n",
        "            # Replace the entire table section\n",
        "            template = template[:start_idx] + per_label_table.strip() + \"\\n\" + template[end_idx:]\n",
        "\n",
        "# Update training data section with actual numbers from README\n",
        "template = template.replace(\"~50,000â€“100,000 (varies by query scope)\", \"64,981 labeled articles (from 76,165 total)\")\n",
        "template = template.replace(\"â‰¤2021 (~60-70%)\", \"â‰¤2021 (29,926 articles, 46.3%)\")\n",
        "template = template.replace(\"2022-2023 (~15-20%)\", \"2022-2023 (16,057 articles, 24.8%)\")\n",
        "template = template.replace(\"â‰¥2024 (~15-20%)\", \"â‰¥2024 (18,666 articles, 28.9%)\")\n",
        "\n",
        "# Update hyperparameters with actual values from training\n",
        "template = template.replace(\"**Learning Rate:** 5e-5\", \"**Learning Rate:** 2e-5\")\n",
        "template = template.replace(\"**Epochs:** 3â€“4\", \"**Epochs:** 3\")\n",
        "\n",
        "# Update hardware specification\n",
        "template = template.replace(\"- **Hardware:** [Specify: e.g., 1x NVIDIA T4, 16GB RAM]\", \n",
        "                            \"- **Hardware:** Apple Silicon (MPS - Metal Performance Shaders) on macOS\")\n",
        "\n",
        "# Add YAML front matter for Hugging Face Hub\n",
        "yaml_front_matter = \"\"\"---\n",
        "library_name: transformers\n",
        "license: mit\n",
        "tags:\n",
        "- multi-label-classification\n",
        "- dental\n",
        "- medical\n",
        "- distilbert\n",
        "- text-classification\n",
        "- evidence-based-medicine\n",
        "- systematic-review\n",
        "task: text-classification\n",
        "datasets:\n",
        "- pubmed\n",
        "metrics:\n",
        "- f1\n",
        "- precision\n",
        "- recall\n",
        "model-index:\n",
        "- name: dental-evidence-triage\n",
        "  results:\n",
        "  - task:\n",
        "      type: text-classification\n",
        "      name: Multi-label Text Classification\n",
        "    dataset:\n",
        "      name: PubMed Dental Abstracts\n",
        "      type: pubmed\n",
        "    metrics:\n",
        "    - type: f1\n",
        "      value: 0.8917\n",
        "      name: Micro-F1\n",
        "    - type: f1\n",
        "      value: 0.7397\n",
        "      name: Macro-F1\n",
        "    - type: precision\n",
        "      value: 0.8966\n",
        "      name: Micro-Precision\n",
        "    - type: recall\n",
        "      value: 0.8868\n",
        "      name: Micro-Recall\n",
        "base_model: distilbert-base-uncased\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Prepend YAML front matter to template\n",
        "template_with_yaml = yaml_front_matter + template\n",
        "\n",
        "# Write to model directory\n",
        "output_path = Path(\"../artifacts/model/best/README.md\")\n",
        "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(output_path, \"w\") as file:\n",
        "    file.write(template_with_yaml)\n",
        "\n",
        "print(f\"âœ… Model card created at {output_path}\")\n",
        "print(f\"   - YAML metadata added\")\n",
        "print(f\"   - Micro-F1: {micro_f1:.4f}\")\n",
        "print(f\"   - Macro-F1: {macro_f1:.4f}\")\n",
        "print(f\"   - Per-label metrics filled for all 10 labels\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Push Model to Hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8dc38da21bda4ae19a960fdf2435da9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1730f9fc1d4949fea513054baf4d90a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model uploaded successfully!\n",
            "   View at: https://huggingface.co/Tuminha/dental-evidence-triage\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: Push model folder to Hugging Face Hub.\n",
        "# Hints:\n",
        "# 1) Create HfApi instance\n",
        "# 2) Create repo (with exist_ok=True)\n",
        "# 3) Upload folder from ../artifacts/model/best\n",
        "# Acceptance:\n",
        "# - Model uploaded successfully\n",
        "# - Accessible at huggingface.co/Tuminha/dental-evidence-triage\n",
        "\n",
        "# TODO: push model\n",
        "hf_api = HfApi()\n",
        "\n",
        "# Create repository (if it doesn't exist)\n",
        "hf_api.create_repo(MODEL_REPO, exist_ok=True)\n",
        "\n",
        "# Upload the model folder\n",
        "# Note: folder_path comes first, then repo_id\n",
        "folder_path = Path(\"../artifacts/model/best\")\n",
        "hf_api.upload_folder(\n",
        "    folder_path=str(folder_path),\n",
        "    repo_id=MODEL_REPO,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(f\"âœ… Model uploaded successfully!\")\n",
        "print(f\"   View at: https://huggingface.co/{MODEL_REPO}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Push Sample Dataset\n",
        "\n",
        "Push a small sample (2-5k rows) for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Sample dataset prepared: 2000 rows\n",
            "   Saved to: ../artifacts/sample_dataset.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df78d31fafd64a0092afc91ba2fe5343",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca4dcc1a73cf4f03babcf4d0d041d899",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Sample dataset uploaded to Tuminha/dental-evidence-dataset\n",
            "   View at: https://huggingface.co/datasets/Tuminha/dental-evidence-dataset\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Goal: (Optional) Push sample dataset for reproducibility.\n",
        "# Hints:\n",
        "# 1) Load train.parquet, sample 2000 rows\n",
        "# 2) Create text column, keep key fields\n",
        "# 3) Save locally, then upload to dataset repo\n",
        "# Acceptance:\n",
        "# - Sample dataset available on HF\n",
        "\n",
        "# TODO: (optional) push sample dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load train.parquet from correct path\n",
        "path_train_parquet = Path(\"../data/processed/train.parquet\")\n",
        "train_df = pd.read_parquet(path_train_parquet)\n",
        "\n",
        "# Sample 2000 rows (with random seed for reproducibility)\n",
        "sample_df = train_df.sample(n=2000, random_state=42)\n",
        "\n",
        "# Create text column (title + abstract)\n",
        "sample_df['text'] = sample_df['title'] + ' ' + sample_df['abstract']\n",
        "sample_df['text'] = sample_df['text'].str[:2000]  # Truncate to 2000 chars (same as training)\n",
        "\n",
        "# Keep key fields: pmid, title, abstract, text, labels, year\n",
        "sample_df = sample_df[['pmid', 'title', 'abstract', 'text', 'labels', 'year']]\n",
        "\n",
        "# Save locally to a temp location\n",
        "path_sample_parquet = Path(\"../artifacts/sample_dataset.parquet\")\n",
        "path_sample_parquet.parent.mkdir(parents=True, exist_ok=True)\n",
        "sample_df.to_parquet(path_sample_parquet)\n",
        "\n",
        "print(f\"âœ… Sample dataset prepared: {len(sample_df)} rows\")\n",
        "print(f\"   Saved to: {path_sample_parquet}\")\n",
        "\n",
        "# Create dataset repository (if it doesn't exist)\n",
        "hf_api.create_repo(DATASET_REPO, repo_type=\"dataset\", exist_ok=True)\n",
        "\n",
        "# Upload sample dataset to Hugging Face Hub\n",
        "hf_api.upload_file(\n",
        "    path_or_fileobj=str(path_sample_parquet),\n",
        "    repo_id=DATASET_REPO,\n",
        "    path_in_repo=\"sample.parquet\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "\n",
        "print(f\"âœ… Sample dataset uploaded to {DATASET_REPO}\")\n",
        "print(f\"   View at: https://huggingface.co/datasets/{DATASET_REPO}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Inference Widget\n",
        "\n",
        "1. Go to https://huggingface.co/Tuminha/dental-evidence-triage\n",
        "2. Settings â†’ Model Card â†’ Add example inputs\n",
        "3. Example:\n",
        "\n",
        "```\n",
        "Title: Effect of chlorhexidine on dental implants: a randomized controlled trial. \n",
        "Abstract: This study evaluated the efficacy of chlorhexidine mouthrinse in preventing peri-implantitis. Sixty patients with dental implants were randomly assigned...\n",
        "```\n",
        "\n",
        "Expected labels: `[RCT, Human]`\n",
        "\n",
        "## Recommendations\n",
        "\n",
        "- **Enable the inference widget** in model settings\n",
        "- **Add label list and examples** to the model card\n",
        "- **Test the widget** with 3-5 diverse abstracts\n",
        "\n",
        "## ðŸ§˜ Reflection Log\n",
        "\n",
        "**What did you learn in this session?**\n",
        "- \n",
        "\n",
        "**What challenges did you encounter?**\n",
        "- \n",
        "\n",
        "**How will this improve Periospot AI?**\n",
        "- \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
